py/nlp/TODO

2016-02-25
    * {README,index}.md need to be committed to github

2016-02-25, edited from -24
    * given ASCII or Unicode text, need to be able to extract 
        Sentences and from them Tokens which include Lexemes 

    * a Lemma is the 'dictionary name' of a Word

    * Lexemes are inflected/conjugated forms of Lemmas, usually or 
        always including the Lemma itself
        - so 'run' is a Lemma, a Verb, and its Lexemes include 'run', 
            'runs', 'running', and 'ran'

    * MultiWords are recognized sequences of words

    * MultiWords include PlaceNames, PersonNames, and EventNames
        - MultiWords (XXX terminology problem! XXX)
            * "data center" aka "datacenter"
            * where the canonical form is all lower case
        - PlaceNames
            * "[San Franciso ]Bay Area" aka "SF Bay Area" 
        - PersonNames 
            * "Joe Biden"
            * where the canonical form may have upper case characters
        - EventNames
            * '9/11'
            * 'Christmas Day', 'Independence Day'
                - where the canonical form includes upper case characters

    * Lexemes include for example "car" and "cars" as well as "car's",
        all seen as variants of the Lemma "car"
        - and similarly "child" and "children"
        - and perhaps "run", "ran", "running"

    * So "San Francisco's ambience" should be recognizable
        - as should "you can't have two San Franciscos"

    * emphasis is on the ability to tokenize technical or semi-technical
        English

    * OK to wrap Natural Language Toolkit (nltk) functions at least
        in the short run
